
# Configuration file for Xeon X5550 Gainestown
# See http://en.wikipedia.org/wiki/Gainestown_(microprocessor)#Gainestown
# and http://ark.intel.com/products/37106

#include nehalem

#[perf_model/l3_cache]
#perfect = false
#cache_block_size = 64
#cache_size = 8192
#associativity = 16
#address_hash = mask
#replacement_policy = lru
#data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
#tags_access_time = 10
#perf_model_type = parallel
#writethrough = 0
#shared_cores = 4

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[perf_model/dram]
type = normal
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
num_controllers = -1
controllers_interleaving = 4
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
latency = 45
per_controller_bandwidth = 7.6              # In GB/s, as measured by core_validation-dram
chips_per_dimm = 8
dimms_per_controller = 4
latency_factor = 1

[perf_model/stacked_dram]
vault_num = 32
bank_num_per_vault = 8
layer_num = 4
row_size = 8192 # Bytes
bandwidth = 256 # Bytes: maximum block size supported in one command

[perf_model/remap_config]
remap_interval = 25 #us
max_remap_time = 5
cross = true
row_access_threshold = 10
invalidation = true
migration = false
mea = true
reactive = false
predictive = false
no_hot_access = false

remap_temp_thres = 83
high_temp_thres = 85
dangerous_temp_thres = 95
init_temp_thres = 65
remap = true
inter_vault = false
n_remap = 1 # number of banks need to be remapped, 0: none(disable), 1: only 1(combine), 2: both(swap)

n_migrate_row = 10

[perf_model/dram_cache]
associativity = 4
page_size = 1984 #B
block_size = 64 #B
addr_map = 3 # 1: vault-bank-row, 2: bank-vault-row, 3: row-bank-vault
offchip_latency = 1

[perf_model/thermal]
sampling_interval = 1000 #us
dtm_method = 0 # 0: no_dvfs, 1: cpu_dvfs, 2: dram_dvfs
temperature_type = 0 #0: average temperature, 1: max temperature
cpu_temp_thres = 100
dram_temp_thres = 80
reverse = false
bank_level_refresh = true
pt_num = 4
freq_num = 5 # length of frequency table
dump_trace = true
record_power = true
hotspot_analysis_threshold = 95
power_scale = -1
default_init_temp = true

[perf_model/core]
frequency = 2.66

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

